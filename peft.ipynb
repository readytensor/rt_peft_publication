{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_count_parameters(model):\n",
    "    \"\"\"\n",
    "    This function takes a PyTorch model and returns the number of trainable and non-trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "    model (torch.nn.Module): The PyTorch model to inspect.\n",
    "\n",
    "    \"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "    percent_trainable = (trainable_params / (trainable_params + non_trainable_params)) * 100\n",
    "    percent_frozen = 100 - percent_trainable\n",
    "\n",
    "    print('Total Parameters:', trainable_params + non_trainable_params)\n",
    "    print('Trainable:', trainable_params, f'({percent_trainable:.2f}%)')\n",
    "    print('Frozen:', non_trainable_params, f'({percent_frozen:.2f}%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Gemma 2B tokenizer and model\n",
    "model_name = \"openai-community/gpt2-large\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Using eos_token as the pad_token if it's not defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True).to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Tokenize the sentences in the dataset during initialization\n",
    "        self.tokenized_data = self.tokenizer(\n",
    "            [example['sentence'] for example in self.dataset],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Store the labels separately\n",
    "        self.labels = [example['label'] for example in self.dataset]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_data['input_ids'][idx]\n",
    "        attention_mask = self.tokenized_data['attention_mask'][idx]\n",
    "        label = self.labels[idx]\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10000 = dataset['train'].select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SST2Dataset(dataset_10000, tokenizer=tokenizer, max_length=32)\n",
    "val_data = SST2Dataset(dataset['validation'], tokenizer=tokenizer, max_length=32)\n",
    "test_data = SST2Dataset(dataset['test'], tokenizer=tokenizer, max_length=32)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 774032640\n",
      "Trainable: 774032640 (100.00%)\n",
      "Frozen: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print_count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs, lr=5e-5, print_loss_per_step=0):\n",
    "    accuracies, losses = [], []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        step = 0\n",
    "        total_loss = 0\n",
    "        predictions, truth_values = [], []\n",
    "        for input_ids, attention_mask, labels in tqdm(train_loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "            predictions.extend(batch_predictions)\n",
    "            truth_values.extend(labels.tolist())\n",
    "\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if print_loss_per_step > 0 and step % print_loss_per_step == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "            step += 1\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        truth_values = np.array(truth_values)\n",
    "        accuracy = np.mean(predictions == truth_values)\n",
    "        print(f\"Epoch {i+1}, Loss: {loss}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print('--'*20)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "    return accuracies, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions, truth_values = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in tqdm(data_loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "            predictions.extend(batch_predictions)\n",
    "            truth_values.extend(labels.tolist())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    truth_values = np.array(truth_values)\n",
    "    accuracy = np.mean(predictions == truth_values)\n",
    "    \n",
    "    return round(float(accuracy), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [05:21<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.31081003419602643\n",
      "Accuracy: 0.8749\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [05:24<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1301158855672176\n",
      "Accuracy: 0.9548\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [05:21<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.06151509812561432\n",
      "Accuracy: 0.9804\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [05:20<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.04552766810485218\n",
      "Accuracy: 0.9874\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [05:25<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.03894183136530766\n",
      "Accuracy: 0.9878\n",
      "----------------------------------------\n",
      "Training time: 1613.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "full_tuning__accuracy, full_tuning_loss = train_model(model, train_loader, epochs=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "full_tuning_time = end - start\n",
    "print(f\"Training time: {full_tuning_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tuning Validation Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"Full Tuning Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine Tuning with peft from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=4,  # Low-rank dimension\n",
    "    lora_alpha=4,  # Alpha scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate to use in LoRA\n",
    "    target_modules=[\"c_attn\"]  # Apply LoRA to attention layers (can be adjusted)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mo/Desktop/peft_approaches/.venv/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = get_peft_model(model, lora_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 774772480\n",
      "Trainable: 739840 (0.10%)\n",
      "Frozen: 774032640 (99.90%)\n"
     ]
    }
   ],
   "source": [
    "print_count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:24<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5962790066537004\n",
      "Accuracy: 0.6726\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:23<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.28547964235559437\n",
      "Accuracy: 0.8833\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:23<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2504016781291261\n",
      "Accuracy: 0.9002\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:23<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.23423901777535963\n",
      "Accuracy: 0.907\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:23<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.21784586601030712\n",
      "Accuracy: 0.9118\n",
      "----------------------------------------\n",
      "Training time: 1016.60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lora_accuracy, lora_loss = train_model(model, train_loader, lr=5e-5, epochs=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "lora_tuning_time = end - start\n",
    "print(f\"Training time: {lora_tuning_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA saved 597.03 seconds (37.00%) in training time.\n"
     ]
    }
   ],
   "source": [
    "saved_time = full_tuning_time - lora_tuning_time\n",
    "saved_percent = (saved_time / full_tuning_time) * 100\n",
    "\n",
    "print(f\"LoRA saved {saved_time:.2f} seconds ({saved_percent:.2f}%) in training time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Validation Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"LoRA Validation Accuracy: {lora_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_layer, alpha, rank=8):\n",
    "        super(LoRA, self).__init__()\n",
    "        \n",
    "        # Store the original layer's weight\n",
    "        self.original_weight = original_layer.weight\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Perform weight decomposition into two low-rank matrices A and B\n",
    "        # We initialize A and B with random values\n",
    "        self.rank = rank\n",
    "\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(original_layer.weight.shape[0], rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, original_layer.weight.shape[1]))\n",
    "        \n",
    "        self.original_weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Approximate the original weight as the product of A and B\n",
    "        low_rank_weight = self.alpha * torch.matmul(self.A, self.B)\n",
    "        adapted_weight = self.original_weight + low_rank_weight\n",
    "        \n",
    "        # Apply the adapted weight to the input\n",
    "        return torch.matmul(x, adapted_weight)\n",
    "    \n",
    "class DoRA(nn.Module):\n",
    "    def __init__(self, original_layer, alpha, rank=8):\n",
    "        super(DoRA, self).__init__()\n",
    "        \n",
    "        # Store the original layer's weight\n",
    "        self.original_weight = original_layer.weight\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Perform weight decomposition into two low-rank matrices A and B\n",
    "        # We initialize A and B with random values\n",
    "        self.rank = rank\n",
    "\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(original_layer.weight.shape[0], rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, original_layer.weight.shape[1]))\n",
    "        self.m = nn.Parameter(torch.ones(1, original_layer.weight.shape[1]))\n",
    "        \n",
    "        self.original_weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Approximate the original weight as the product of A and B\n",
    "        low_rank_weight = self.alpha * torch.matmul(self.A, self.B)\n",
    "\n",
    "        low_rank_weight_norm = low_rank_weight / (low_rank_weight.norm(p=2, dim=1, keepdim=True) + 1e-9)\n",
    "        \n",
    "        # Add the original (frozen) weight back to the low-rank adaptation\n",
    "        low_rank_weight = self.m * low_rank_weight_norm\n",
    "        adapted_weight = self.original_weight + low_rank_weight\n",
    "        \n",
    "        # Apply the adapted weight to the input\n",
    "        return torch.matmul(x, adapted_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import Conv1D\n",
    "def apply_peft_to_layer(module, alpha=4, rank=8, type='lora'):\n",
    "    \"\"\"\n",
    "    Recursively applies LoRA/DoRA to the appropriate layers in the model.\n",
    "    \n",
    "    Args:\n",
    "        module: The current module to examine and possibly replace.\n",
    "        alpha: Scaling factor for DoRA.\n",
    "        rank: The rank of the low-rank adaptation.\n",
    "    \n",
    "    Returns:\n",
    "        None (modifies the module in place).\n",
    "    \"\"\"\n",
    "    peft_module = LoRA if type == 'lora' else DoRA\n",
    "    for name, child_module in module.named_children():        \n",
    "        if isinstance(child_module, Conv1D) and 'c_attn' in name:\n",
    "            # Replace with DoRA version of the module\n",
    "            setattr(module, name, peft_module(child_module, alpha=alpha, rank=rank))\n",
    "        \n",
    "        # If the module has children, apply the function recursively\n",
    "        if len(list(child_module.children())) > 0:\n",
    "            apply_peft_to_layer(child_module, alpha, rank, type)\n",
    "\n",
    "def get_peft_model(alpha=4, rank=8, type='lora'):\n",
    "    \"\"\"\n",
    "    Load the model and apply LoRA/DoRA recursively to all applicable layers.\n",
    "    \n",
    "    Args:\n",
    "        model_name: The name of the model to load.\n",
    "        alpha: Scaling factor for DoRA.\n",
    "        rank: Rank for low-rank adaptation in DoRA.\n",
    "    \n",
    "    Returns:\n",
    "        The model with LoRA/DoRA applied.\n",
    "    \"\"\"\n",
    "    # Load the model and set the pad token ID\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True).to(device)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Apply DoRA recursively to all relevant layers\n",
    "    apply_peft_to_layer(model, alpha=alpha, rank=rank, type=type)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LoRA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 774631680\n",
      "Trainable: 737280 (0.10%)\n",
      "Frozen: 773894400 (99.90%)\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(alpha=4, rank=4, type='lora').to(device)\n",
    "print_count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3316581709125933\n",
      "Accuracy: 0.8568\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.21332219677468459\n",
      "Accuracy: 0.9179\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1741975861908005\n",
      "Accuracy: 0.9354\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.14268856493238444\n",
      "Accuracy: 0.9473\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:33<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.12055501734284452\n",
      "Accuracy: 0.9543\n",
      "----------------------------------------\n",
      "Training time: 1070.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "custom_lora_accuracy, custom_lora_loss = train_model(model, train_loader, epochs=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "custom_lora_tuning_time = end - start\n",
    "print(f\"Training time: {custom_lora_tuning_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom LoRA saved 542.84 seconds (33.64%) in training time.\n"
     ]
    }
   ],
   "source": [
    "saved_time = full_tuning_time - custom_lora_tuning_time\n",
    "saved_percent = (saved_time / full_tuning_time) * 100\n",
    "\n",
    "print(f\"Custom LoRA saved {saved_time:.2f} seconds ({saved_percent:.2f}%) in training time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom LoRA Validation Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "custom_lora_val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"Custom LoRA Validation Accuracy: {lora_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 774769920\n",
      "Trainable: 875520 (0.11%)\n",
      "Frozen: 773894400 (99.89%)\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(alpha=4, rank=4, type='dora').to(device)\n",
    "print_count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:01<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5197267194335072\n",
      "Accuracy: 0.7414\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.28997675876933543\n",
      "Accuracy: 0.8772\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.22610754893420223\n",
      "Accuracy: 0.9093\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.185149262888363\n",
      "Accuracy: 0.9289\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [04:00<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1505310721397876\n",
      "Accuracy: 0.9401\n",
      "----------------------------------------\n",
      "Training time: 1205.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "dora_accuracy, dora_loss = train_model(model, train_loader, lr=0.01, epochs=5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "dora_tuning_time = end - start\n",
    "print(f\"Training time: {dora_tuning_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoRA saved 408.12 seconds (25.29%) in training time.\n"
     ]
    }
   ],
   "source": [
    "saved_time = full_tuning_time - dora_tuning_time\n",
    "saved_percent = (saved_time / full_tuning_time) * 100\n",
    "\n",
    "print(f\"DoRA saved {saved_time:.2f} seconds ({saved_percent:.2f}%) in training time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:10<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoRA Validation Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dora_val_accuracy = evaluate_model(model, val_loader)\n",
    "print(f\"DoRA Validation Accuracy: {dora_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
